{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPKZMTSSSivj",
    "outputId": "abdd1516-f6ca-4b12-d7b0-444bbfa080f9"
   },
   "outputs": [],
   "source": [
    "# Download dataset. This is not needed if you have the dataset already\n",
    "\n",
    "# import kagglehub\n",
    "\n",
    "# path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SnlbjELrSivk",
    "outputId": "798e7ac8-364b-4ba1-caa3-c1601f3584bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate: 48000 Hz\n"
     ]
    }
   ],
   "source": [
    "# Function to check the sampling rate of a wav file and valid file path\n",
    "\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "def check_sampling_rate(file_path):\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(file_path, 'r')) as wav_file:\n",
    "            sample_rate = wav_file.getframerate()\n",
    "            print(f\"Sampling rate: {sample_rate} Hz\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "file_path = 'data/ravdess-emotional-speech-audio/versions/1/Actor_01/03-01-01-01-01-01-01.wav'\n",
    "check_sampling_rate(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1l5AM4UkSivk",
    "outputId": "dba92514-0d9b-4bfc-c309-37cc31b2846f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        emotion                          file_path\n",
      "0       disgust  Actor_24_03-01-07-01-02-01-24.wav\n",
      "1     surprised  Actor_24_03-01-08-02-01-01-24.wav\n",
      "2           sad  Actor_24_03-01-04-01-02-01-24.wav\n",
      "3         angry  Actor_24_03-01-05-01-01-01-24.wav\n",
      "4           sad  Actor_24_03-01-04-02-01-01-24.wav\n",
      "...         ...                                ...\n",
      "1435    neutral  Actor_07_03-01-01-01-01-01-07.wav\n",
      "1436      happy  Actor_07_03-01-03-02-01-02-07.wav\n",
      "1437    neutral  Actor_07_03-01-01-01-01-02-07.wav\n",
      "1438    disgust  Actor_07_03-01-07-02-01-02-07.wav\n",
      "1439       calm  Actor_07_03-01-02-02-01-01-07.wav\n",
      "\n",
      "[1440 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "File naming convention\n",
    "\n",
    "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
    "\n",
    "Filename identifiers\n",
    "\n",
    "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "\n",
    "Vocal channel (01 = speech, 02 = song).\n",
    "\n",
    "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "\n",
    "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "\n",
    "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "\n",
    "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "emotion_mapping = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "file_dir = \"data/ravdess-emotional-speech-audio/versions/1/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for actor in os.listdir(file_dir):\n",
    "    actor_path = os.path.join(file_dir, actor)\n",
    "    \n",
    "    if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
    "        actor_number = actor.split(\"_\")[-1]\n",
    "\n",
    "        for file in os.listdir(actor_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                emotion_code = file[6:8]\n",
    "                emotion = emotion_mapping.get(emotion_code, \"unknown\")\n",
    "                formatted_filename = f\"Actor_{actor_number}_{file}\"\n",
    "                data.append({\"emotion\": emotion, \"file_path\": formatted_filename})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove calm emotion\n",
    "df = df[df.emotion != 'calm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting waveforms and spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vQvjb4mASivk"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from matplotlib.patches import Rectangle\n",
    "from torchaudio.utils import download_asset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "def plot_waveform(waveform, sr, title=None, ax=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(num_channels, 1)\n",
    "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "    ax.set_xlim([0, time_axis[-1]])\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_wave_and_spec():\n",
    "    base_dir = \"data/ravdess-emotional-speech-audio/versions/1\"\n",
    "    output_dir = \"speech\"\n",
    "\n",
    "    for actor in os.listdir(base_dir):\n",
    "        actor_path = os.path.join(base_dir, actor)\n",
    "        if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
    "            actor_num = int(actor.split(\"_\")[1])\n",
    "            if actor_num > 22: # change actor number here because it keeps crashing midway\n",
    "                print(f\"Processing {actor}...\")\n",
    "                for file in os.listdir(actor_path):\n",
    "                    if file.endswith(\".wav\"):\n",
    "                        SAMPLE_SPEECH = os.path.join(actor_path, file)\n",
    "                        SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_SPEECH)\n",
    "\n",
    "                        # Define transform\n",
    "                        spectrogram = T.Spectrogram(n_fft=512)\n",
    "\n",
    "                        # Perform transform\n",
    "                        spec = spectrogram(SPEECH_WAVEFORM)\n",
    "                        \n",
    "                        fig, ax = plt.subplots()\n",
    "                        plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=None, ax=ax)\n",
    "                        waveform_path = os.path.join(output_dir, f\"{actor}_{file}_waveform.png\")\n",
    "                        plt.savefig(waveform_path)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                        # Create figure for spectrogram\n",
    "                        fig, ax = plt.subplots()\n",
    "                        plot_spectrogram(spec[0], title=None, ax=ax)\n",
    "                        spectrogram_path = os.path.join(output_dir, f\"{actor}_{file}_spectrogram.png\")\n",
    "                        plt.savefig(spectrogram_path)\n",
    "                        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting speech waveforms and spectogram to pd dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emotion                          file_path  \\\n",
      "0    disgust  Actor_24_03-01-07-01-02-01-24.wav   \n",
      "1  surprised  Actor_24_03-01-08-02-01-01-24.wav   \n",
      "2        sad  Actor_24_03-01-04-01-02-01-24.wav   \n",
      "3      angry  Actor_24_03-01-05-01-01-01-24.wav   \n",
      "4        sad  Actor_24_03-01-04-02-01-01-24.wav   \n",
      "\n",
      "                                  spectrogram_tensor  \n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define base path for spectrogram images\n",
    "image_dir = \"speech\"  # Directory where spectrogram images are stored\n",
    "\n",
    "# Define transformations (convert images to tensors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
    "])\n",
    "\n",
    "# Function to load spectrogram as tensor\n",
    "def load_spectrogram_tensor(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    spectrogram_img_path = os.path.join(image_dir, f\"{filename}_spectrogram.png\")  # Construct spectrogram path\n",
    "    \n",
    "    # Load image if it exists, else return None\n",
    "    if os.path.exists(spectrogram_img_path):\n",
    "        image = Image.open(spectrogram_img_path).convert(\"L\")  # Convert to grayscale\n",
    "        return transform(image)  # Convert to tensor\n",
    "    return None  # If file doesn't exist, return None\n",
    "\n",
    "# Apply function to extract spectrogram tensors\n",
    "df[\"spectrogram_tensor\"] = df[\"file_path\"].apply(load_spectrogram_tensor)\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emotion                          file_path  \\\n",
      "0    disgust  Actor_24_03-01-07-01-02-01-24.wav   \n",
      "1  surprised  Actor_24_03-01-08-02-01-01-24.wav   \n",
      "2        sad  Actor_24_03-01-04-01-02-01-24.wav   \n",
      "3      angry  Actor_24_03-01-05-01-01-01-24.wav   \n",
      "4        sad  Actor_24_03-01-04-02-01-01-24.wav   \n",
      "\n",
      "                                  spectrogram_tensor  \\\n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "\n",
      "                                     waveform_tensor  \n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base path for waveform images\n",
    "image_dir = \"speech\"  # Directory where waveform images are stored\n",
    "\n",
    "# Define transformations (convert images to tensors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
    "])\n",
    "\n",
    "# Function to load waveform as tensor\n",
    "def load_waveform_tensor(file_path):\n",
    "    filename = os.path.basename(file_path)  # Extract filename (e.g., \"Actor_01_03-01-01-01-01-01-01.wav\")\n",
    "    waveform_img_path = os.path.join(image_dir, f\"{filename}_waveform.png\")  # Construct waveform path\n",
    "    \n",
    "    # Load image if it exists, else return None\n",
    "    if os.path.exists(waveform_img_path):\n",
    "        image = Image.open(waveform_img_path).convert(\"L\")  # Convert to grayscale\n",
    "        return transform(image)  # Convert to tensor\n",
    "    return None  # If file doesn't exist, return None\n",
    "\n",
    "# Apply function to extract waveform tensors\n",
    "df[\"waveform_tensor\"] = df[\"file_path\"].apply(load_waveform_tensor)\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "disgust      192\n",
       "surprised    192\n",
       "sad          192\n",
       "angry        192\n",
       "fearful      192\n",
       "happy        192\n",
       "neutral       96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 480, 640])\n",
      "torch.Size([1, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0, 2].shape)\n",
    "print(df.iloc[0, 3].shape)\n",
    "\n",
    "#combined = torch.cat([tensor1, tensor2], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['emotion'], random_state=42)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0]['spectrogram_tensor']\n",
    "\n",
    "emotion_to_num = {\n",
    "    \"neutral\": 0,\n",
    "    \"happy\": 1,\n",
    "    \"sad\": 2,\n",
    "    \"angry\": 3,\n",
    "    \"fearful\": 4,\n",
    "    \"disgust\": 5,\n",
    "    \"surprised\": 6\n",
    "}\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "#create train set \n",
    "for i in range(len(train_df)):\n",
    "  #concat the spectogram and waveform tensors into one \n",
    "  combined = torch.cat([train_df.iloc[i]['spectrogram_tensor'], train_df.iloc[i]['waveform_tensor']], dim=0)\n",
    "  label_tensor = torch.tensor(emotion_to_num[train_df.iloc[i]['emotion']], dtype=torch.long)\n",
    "  #save to dataset \n",
    "  train_set.append((combined, label_tensor))\n",
    "\n",
    "#create test set \n",
    "for i in range(len(test_df)):\n",
    "  #concat the spectogram and waveform tensors into one \n",
    "  combined = torch.cat([test_df.iloc[i]['spectrogram_tensor'], test_df.iloc[i]['waveform_tensor']], dim=0)\n",
    "  label_tensor = torch.tensor(emotion_to_num[test_df.iloc[i]['emotion']], dtype=torch.long)\n",
    "  test_set.append((combined, label_tensor))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16  # <-- Please change this as necessary\n",
    "NUM_WORKERS = 4  # <-- Use more workers for more CPU threads\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 120 * 160, 128)  # Fixed input size\n",
    "        self.fc2 = nn.Linear(128, 7)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv + ReLU + Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv + ReLU + Pooling\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten for the FC layer\n",
    "        x = F.relu(self.fc1(x))  # Hidden layer\n",
    "        x = self.dropout(x)  # Apply dropout after the fully connected layer\n",
    "        x = self.fc2(x)  # Output logits (no softmax needed before CrossEntropyLoss)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "def train(train_loader, model, optimizer, criterion, n_epochs=10, **kwargs):\n",
    "    ### Define your training loop here\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i in tqdm(range(n_epochs)):\n",
    "          print(i)\n",
    "          batch_losses = []\n",
    "          correct = 0  \n",
    "          total = 0\n",
    "          lastepoch_loss = 0\n",
    "        \n",
    "          for j, data in enumerate(train_loader, 0):  \n",
    "            \n",
    "            x_batch, y_batch = data\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)  \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_predictions = model(x_batch)\n",
    "            loss = criterion(y_predictions, y_batch)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "              \n",
    "            _, predicted = torch.max(y_predictions, 1)  # Get class with highest score\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "          epoch_loss = np.mean(batch_losses)\n",
    "          losses.append(epoch_loss)\n",
    "          epoch_accuracy = 100 * correct / total\n",
    "\n",
    "          print(f\"Train Loss: {losses[-1]:.4f} | Train Accuracy: {epoch_accuracy:.2f}%\\n\")\n",
    "\n",
    "          #break out of for loop if the loss is not improving significantly \n",
    "          if abs(lastepoch_loss-epoch_loss) < 0.01: \n",
    "              print(\"stopping early\")\n",
    "              break\n",
    "          lastepoch_loss = epoch_loss \n",
    "        \n",
    "    return losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "mymodel = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mymodel.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.0087 | Train Accuracy: 18.54%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 3\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmymodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, criterion, n_epochs, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#break out of for loop if the loss is not improving significantly \u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlosses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m-\u001b[39mepoch_loss \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.01\u001b[39m: \n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopping early\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "losses = train(train_loader, mymodel, optimizer, criterion, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c0c6dbaafb0>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(losses)), losses, label='Loss Plot', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # We don't need gradients for evaluation\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)  # Increment the total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Accuracy as a percentage\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 26.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(mymodel, test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
