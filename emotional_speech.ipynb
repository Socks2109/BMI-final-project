{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPKZMTSSSivj",
    "outputId": "abdd1516-f6ca-4b12-d7b0-444bbfa080f9"
   },
   "outputs": [],
   "source": [
    "# Download dataset. This is not needed if you have the dataset already\n",
    "\n",
    "# import kagglehub\n",
    "\n",
    "# path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SnlbjELrSivk",
    "outputId": "798e7ac8-364b-4ba1-caa3-c1601f3584bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate: 48000 Hz\n"
     ]
    }
   ],
   "source": [
    "# Function to check the sampling rate of a wav file and valid file path\n",
    "\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "def check_sampling_rate(file_path):\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(file_path, 'r')) as wav_file:\n",
    "            sample_rate = wav_file.getframerate()\n",
    "            print(f\"Sampling rate: {sample_rate} Hz\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "file_path = 'data/ravdess-emotional-speech-audio/versions/1/Actor_01/03-01-01-01-01-01-01.wav'\n",
    "check_sampling_rate(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1l5AM4UkSivk",
    "outputId": "dba92514-0d9b-4bfc-c309-37cc31b2846f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        emotion                          file_path\n",
      "0       disgust  Actor_24_03-01-07-01-02-01-24.wav\n",
      "1     surprised  Actor_24_03-01-08-02-01-01-24.wav\n",
      "2           sad  Actor_24_03-01-04-01-02-01-24.wav\n",
      "3         angry  Actor_24_03-01-05-01-01-01-24.wav\n",
      "4           sad  Actor_24_03-01-04-02-01-01-24.wav\n",
      "...         ...                                ...\n",
      "1435    neutral  Actor_07_03-01-01-01-01-01-07.wav\n",
      "1436      happy  Actor_07_03-01-03-02-01-02-07.wav\n",
      "1437    neutral  Actor_07_03-01-01-01-01-02-07.wav\n",
      "1438    disgust  Actor_07_03-01-07-02-01-02-07.wav\n",
      "1439       calm  Actor_07_03-01-02-02-01-01-07.wav\n",
      "\n",
      "[1440 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "File naming convention\n",
    "\n",
    "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
    "\n",
    "Filename identifiers\n",
    "\n",
    "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "\n",
    "Vocal channel (01 = speech, 02 = song).\n",
    "\n",
    "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "\n",
    "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "\n",
    "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "\n",
    "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "emotion_mapping = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "file_dir = \"data/ravdess-emotional-speech-audio/versions/1/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for actor in os.listdir(file_dir):\n",
    "    actor_path = os.path.join(file_dir, actor)\n",
    "    \n",
    "    if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
    "        actor_number = actor.split(\"_\")[-1]\n",
    "\n",
    "        for file in os.listdir(actor_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                emotion_code = file[6:8]\n",
    "                emotion = emotion_mapping.get(emotion_code, \"unknown\")\n",
    "                formatted_filename = f\"Actor_{actor_number}_{file}\"\n",
    "                data.append({\"emotion\": emotion, \"file_path\": formatted_filename})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove calm emotion\n",
    "df = df[df.emotion != 'calm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting waveforms and spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vQvjb4mASivk"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from matplotlib.patches import Rectangle\n",
    "from torchaudio.utils import download_asset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "def plot_waveform(waveform, sr, title=None, ax=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(num_channels, 1)\n",
    "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "    ax.set_xlim([0, time_axis[-1]])\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_wave_and_spec():\n",
    "    base_dir = \"data/ravdess-emotional-speech-audio/versions/1\"\n",
    "    output_dir = \"speech\"\n",
    "\n",
    "    for actor in os.listdir(base_dir):\n",
    "        actor_path = os.path.join(base_dir, actor)\n",
    "        if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
    "            actor_num = int(actor.split(\"_\")[1])\n",
    "            if actor_num > 22: # change actor number here because it keeps crashing midway\n",
    "                print(f\"Processing {actor}...\")\n",
    "                for file in os.listdir(actor_path):\n",
    "                    if file.endswith(\".wav\"):\n",
    "                        SAMPLE_SPEECH = os.path.join(actor_path, file)\n",
    "                        SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_SPEECH)\n",
    "\n",
    "                        # Define transform\n",
    "                        spectrogram = T.Spectrogram(n_fft=512)\n",
    "\n",
    "                        # Perform transform\n",
    "                        spec = spectrogram(SPEECH_WAVEFORM)\n",
    "                        \n",
    "                        fig, ax = plt.subplots()\n",
    "                        plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=None, ax=ax)\n",
    "                        waveform_path = os.path.join(output_dir, f\"{actor}_{file}_waveform.png\")\n",
    "                        plt.savefig(waveform_path)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                        # Create figure for spectrogram\n",
    "                        fig, ax = plt.subplots()\n",
    "                        plot_spectrogram(spec[0], title=None, ax=ax)\n",
    "                        spectrogram_path = os.path.join(output_dir, f\"{actor}_{file}_spectrogram.png\")\n",
    "                        plt.savefig(spectrogram_path)\n",
    "                        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting speech waveforms and spectogram to pd dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emotion                          file_path  \\\n",
      "0    disgust  Actor_24_03-01-07-01-02-01-24.wav   \n",
      "1  surprised  Actor_24_03-01-08-02-01-01-24.wav   \n",
      "2        sad  Actor_24_03-01-04-01-02-01-24.wav   \n",
      "3      angry  Actor_24_03-01-05-01-01-01-24.wav   \n",
      "4        sad  Actor_24_03-01-04-02-01-01-24.wav   \n",
      "\n",
      "                                  spectrogram_tensor  \n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define base path for spectrogram images\n",
    "image_dir = \"speech\"  # Directory where spectrogram images are stored\n",
    "\n",
    "# Define transformations (convert images to tensors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
    "])\n",
    "\n",
    "# Function to load spectrogram as tensor\n",
    "def load_spectrogram_tensor(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    spectrogram_img_path = os.path.join(image_dir, f\"{filename}_spectrogram.png\")  # Construct spectrogram path\n",
    "    \n",
    "    # Load image if it exists, else return None\n",
    "    if os.path.exists(spectrogram_img_path):\n",
    "        image = Image.open(spectrogram_img_path).convert(\"L\")  # Convert to grayscale\n",
    "        return transform(image)  # Convert to tensor\n",
    "    return None  # If file doesn't exist, return None\n",
    "\n",
    "# Apply function to extract spectrogram tensors\n",
    "df[\"spectrogram_tensor\"] = df[\"file_path\"].apply(load_spectrogram_tensor)\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emotion                          file_path  \\\n",
      "0    disgust  Actor_24_03-01-07-01-02-01-24.wav   \n",
      "1  surprised  Actor_24_03-01-08-02-01-01-24.wav   \n",
      "2        sad  Actor_24_03-01-04-01-02-01-24.wav   \n",
      "3      angry  Actor_24_03-01-05-01-01-01-24.wav   \n",
      "4        sad  Actor_24_03-01-04-02-01-01-24.wav   \n",
      "\n",
      "                                  spectrogram_tensor  \\\n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "\n",
      "                                     waveform_tensor  \n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base path for waveform images\n",
    "image_dir = \"speech\"  # Directory where waveform images are stored\n",
    "\n",
    "# Define transformations (convert images to tensors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
    "])\n",
    "\n",
    "# Function to load waveform as tensor\n",
    "def load_waveform_tensor(file_path):\n",
    "    filename = os.path.basename(file_path)  # Extract filename (e.g., \"Actor_01_03-01-01-01-01-01-01.wav\")\n",
    "    waveform_img_path = os.path.join(image_dir, f\"{filename}_waveform.png\")  # Construct waveform path\n",
    "    \n",
    "    # Load image if it exists, else return None\n",
    "    if os.path.exists(waveform_img_path):\n",
    "        image = Image.open(waveform_img_path).convert(\"L\")  # Convert to grayscale\n",
    "        return transform(image)  # Convert to tensor\n",
    "    return None  # If file doesn't exist, return None\n",
    "\n",
    "# Apply function to extract waveform tensors\n",
    "df[\"waveform_tensor\"] = df[\"file_path\"].apply(load_waveform_tensor)\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "disgust      192\n",
       "surprised    192\n",
       "sad          192\n",
       "angry        192\n",
       "fearful      192\n",
       "happy        192\n",
       "neutral       96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 480, 640])\n",
      "torch.Size([1, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0, 2].shape)\n",
    "print(df.iloc[0, 3].shape)\n",
    "\n",
    "#combined = torch.cat([tensor1, tensor2], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['emotion'], random_state=42)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0]['spectrogram_tensor']\n",
    "\n",
    "emotion_to_num = {\n",
    "    \"neutral\": 0,\n",
    "    \"happy\": 1,\n",
    "    \"sad\": 2,\n",
    "    \"angry\": 3,\n",
    "    \"fearful\": 4,\n",
    "    \"disgust\": 5,\n",
    "    \"surprised\": 6\n",
    "}\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "#create train set \n",
    "for i in range(len(train_df)):\n",
    "  #concat the spectogram and waveform tensors into one \n",
    "  combined = torch.cat([train_df.iloc[i]['spectrogram_tensor'], train_df.iloc[i]['waveform_tensor']], dim=0)\n",
    "  label_tensor = torch.tensor(emotion_to_num[train_df.iloc[i]['emotion']], dtype=torch.long)\n",
    "  #save to dataset \n",
    "  train_set.append((combined, label_tensor))\n",
    "\n",
    "#create test set \n",
    "for i in range(len(test_df)):\n",
    "  #concat the spectogram and waveform tensors into one \n",
    "  combined = torch.cat([test_df.iloc[i]['spectrogram_tensor'], test_df.iloc[i]['waveform_tensor']], dim=0)\n",
    "  label_tensor = torch.tensor(emotion_to_num[test_df.iloc[i]['emotion']], dtype=torch.long)\n",
    "  test_set.append((combined, label_tensor))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16  # <-- Please change this as necessary\n",
    "NUM_WORKERS = 4  # <-- Use more workers for more CPU threads\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 120 * 160, 128)  # Fixed input size\n",
    "        self.fc2 = nn.Linear(128, 7)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv + ReLU + Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv + ReLU + Pooling\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten for the FC layer\n",
    "        x = F.relu(self.fc1(x))  # Hidden layer\n",
    "        x = self.dropout(x)  # Apply dropout after the fully connected layer\n",
    "        x = self.fc2(x)  # Output logits (no softmax needed before CrossEntropyLoss)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "def train(train_loader, model, optimizer, criterion, n_epochs=10, **kwargs):\n",
    "    ### Define your training loop here\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i in tqdm(range(n_epochs)):\n",
    "          print(i)\n",
    "          batch_losses = []\n",
    "          correct = 0  \n",
    "          total = 0\n",
    "          lastepoch_loss = 0\n",
    "        \n",
    "          for j, data in enumerate(train_loader, 0):  \n",
    "            \n",
    "            x_batch, y_batch = data\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)  \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_predictions = model(x_batch)\n",
    "            loss = criterion(y_predictions, y_batch)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "              \n",
    "            _, predicted = torch.max(y_predictions, 1)  # Get class with highest score\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "          epoch_loss = np.mean(batch_losses)\n",
    "          losses.append(epoch_loss)\n",
    "          epoch_accuracy = 100 * correct / total\n",
    "\n",
    "          print(f\"Train Loss: {losses[-1]:.4f} | Train Accuracy: {epoch_accuracy:.2f}%\\n\")\n",
    "\n",
    "          #break out of for loop if the loss is not improving significantly \n",
    "          if abs(lastepoch_loss-epoch_loss) < 0.01: \n",
    "              print(\"stopping early\")\n",
    "              break\n",
    "          lastepoch_loss = epoch_loss \n",
    "        \n",
    "    return losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "mymodel = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mymodel.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:06<05:28,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.2655 | Train Accuracy: 13.93%\n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:13<05:25,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9369 | Train Accuracy: 15.33%\n",
      "\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:20<05:15,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9204 | Train Accuracy: 18.74%\n",
      "\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:26<05:10,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8610 | Train Accuracy: 24.45%\n",
      "\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:33<05:04,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7601 | Train Accuracy: 31.86%\n",
      "\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:40<04:57,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6155 | Train Accuracy: 35.77%\n",
      "\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:47<04:49,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4664 | Train Accuracy: 42.99%\n",
      "\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:53<04:43,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4064 | Train Accuracy: 45.29%\n",
      "\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [01:00<04:35,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3184 | Train Accuracy: 47.09%\n",
      "\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:07<04:29,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1531 | Train Accuracy: 57.72%\n",
      "\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [01:14<04:23,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0525 | Train Accuracy: 59.52%\n",
      "\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [01:21<04:17,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9368 | Train Accuracy: 62.83%\n",
      "\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [01:27<04:09,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8815 | Train Accuracy: 66.63%\n",
      "\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [01:34<04:03,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7956 | Train Accuracy: 70.24%\n",
      "\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [01:41<03:57,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6836 | Train Accuracy: 72.34%\n",
      "\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [01:48<03:50,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6063 | Train Accuracy: 76.55%\n",
      "\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [01:54<03:43,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5463 | Train Accuracy: 78.56%\n",
      "\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [02:01<03:37,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5414 | Train Accuracy: 79.46%\n",
      "\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [02:08<03:29,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4682 | Train Accuracy: 80.36%\n",
      "\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [02:15<03:22,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4582 | Train Accuracy: 80.56%\n",
      "\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [02:21<03:16,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4334 | Train Accuracy: 81.66%\n",
      "\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [02:28<03:09,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4218 | Train Accuracy: 83.17%\n",
      "\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [02:35<03:03,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4300 | Train Accuracy: 82.67%\n",
      "\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [02:42<02:56,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3726 | Train Accuracy: 84.77%\n",
      "\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [02:49<02:49,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3902 | Train Accuracy: 83.37%\n",
      "\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [02:55<02:42,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3273 | Train Accuracy: 85.97%\n",
      "\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [03:02<02:35,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3324 | Train Accuracy: 86.87%\n",
      "\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [03:09<02:28,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3292 | Train Accuracy: 86.27%\n",
      "\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [03:16<02:21,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3054 | Train Accuracy: 87.37%\n",
      "\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [03:22<02:15,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2607 | Train Accuracy: 88.58%\n",
      "\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [03:29<02:08,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2940 | Train Accuracy: 87.98%\n",
      "\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [03:36<02:01,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2660 | Train Accuracy: 88.28%\n",
      "\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [03:43<01:54,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2802 | Train Accuracy: 87.98%\n",
      "\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [03:49<01:47,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2718 | Train Accuracy: 88.58%\n",
      "\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [03:56<01:40,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2809 | Train Accuracy: 87.88%\n",
      "\n",
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [04:03<01:34,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2860 | Train Accuracy: 88.28%\n",
      "\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [04:10<01:28,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2749 | Train Accuracy: 88.28%\n",
      "\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [04:16<01:21,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2152 | Train Accuracy: 92.08%\n",
      "\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [04:23<01:14,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2506 | Train Accuracy: 89.68%\n",
      "\n",
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [04:30<01:07,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2611 | Train Accuracy: 88.38%\n",
      "\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [04:37<01:00,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2484 | Train Accuracy: 89.48%\n",
      "\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [04:43<00:53,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2571 | Train Accuracy: 89.08%\n",
      "\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [04:50<00:47,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2248 | Train Accuracy: 90.78%\n",
      "\n",
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [04:57<00:40,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2481 | Train Accuracy: 89.18%\n",
      "\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [05:04<00:33,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2331 | Train Accuracy: 90.28%\n",
      "\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [05:10<00:26,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2339 | Train Accuracy: 89.68%\n",
      "\n",
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [05:17<00:20,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2044 | Train Accuracy: 91.68%\n",
      "\n",
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [05:24<00:13,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2130 | Train Accuracy: 91.48%\n",
      "\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [05:31<00:06,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2012 | Train Accuracy: 91.08%\n",
      "\n",
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:38<00:00,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2275 | Train Accuracy: 89.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "losses = train(train_loader, mymodel, optimizer, criterion, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c0c6d4ac5e0>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(losses)), losses, label='Loss Plot', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # We don't need gradients for evaluation\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)  # Increment the total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Accuracy as a percentage\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 31.20%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(mymodel, test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
