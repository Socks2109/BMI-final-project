{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPKZMTSSSivj",
        "outputId": "abdd1516-f6ca-4b12-d7b0-444bbfa080f9"
      },
      "outputs": [],
      "source": [
        "# Download dataset. This is not needed if you have the dataset already\n",
        "\n",
        "# import kagglehub\n",
        "\n",
        "# path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "\n",
        "# print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning and Reformatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SnlbjELrSivk",
        "outputId": "798e7ac8-364b-4ba1-caa3-c1601f3584bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling rate: 48000 Hz\n"
          ]
        }
      ],
      "source": [
        "# Function to check the sampling rate of a wav file and valid file path\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "def check_sampling_rate(file_path):\n",
        "    try:\n",
        "        with contextlib.closing(wave.open(file_path, 'r')) as wav_file:\n",
        "            sample_rate = wav_file.getframerate()\n",
        "            print(f\"Sampling rate: {sample_rate} Hz\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "file_path = 'data/ravdess-emotional-speech-audio/versions/1/Actor_01/03-01-01-01-01-01-01.wav'\n",
        "check_sampling_rate(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1l5AM4UkSivk",
        "outputId": "dba92514-0d9b-4bfc-c309-37cc31b2846f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      emotion                          file_path\n",
            "0       angry  Actor_16_03-01-05-01-02-01-16.wav\n",
            "1     fearful  Actor_16_03-01-06-01-02-02-16.wav\n",
            "2     fearful  Actor_16_03-01-06-02-01-02-16.wav\n",
            "3       angry  Actor_16_03-01-05-02-01-01-16.wav\n",
            "4     disgust  Actor_16_03-01-07-01-01-01-16.wav\n",
            "...       ...                                ...\n",
            "1435    happy  Actor_08_03-01-03-02-02-02-08.wav\n",
            "1436    happy  Actor_08_03-01-03-01-01-02-08.wav\n",
            "1437     calm  Actor_08_03-01-02-02-01-01-08.wav\n",
            "1438     calm  Actor_08_03-01-02-01-02-01-08.wav\n",
            "1439  neutral  Actor_08_03-01-01-01-02-02-08.wav\n",
            "\n",
            "[1440 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "File naming convention\n",
        "\n",
        "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
        "\n",
        "Filename identifiers\n",
        "\n",
        "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "\n",
        "Vocal channel (01 = speech, 02 = song).\n",
        "\n",
        "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "\n",
        "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "\n",
        "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "\n",
        "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "emotion_mapping = {\n",
        "    \"01\": \"neutral\",\n",
        "    \"02\": \"calm\",\n",
        "    \"03\": \"happy\",\n",
        "    \"04\": \"sad\",\n",
        "    \"05\": \"angry\",\n",
        "    \"06\": \"fearful\",\n",
        "    \"07\": \"disgust\",\n",
        "    \"08\": \"surprised\"\n",
        "}\n",
        "\n",
        "file_dir = \"data/ravdess-emotional-speech-audio/versions/1/\"\n",
        "\n",
        "data = []\n",
        "\n",
        "for actor in os.listdir(file_dir):\n",
        "    actor_path = os.path.join(file_dir, actor)\n",
        "    \n",
        "    if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
        "        actor_number = actor.split(\"_\")[-1]\n",
        "\n",
        "        for file in os.listdir(actor_path):\n",
        "            if file.endswith(\".wav\"):\n",
        "                emotion_code = file[6:8]\n",
        "                emotion = emotion_mapping.get(emotion_code, \"unknown\")\n",
        "                formatted_filename = f\"Actor_{actor_number}_{file}\"\n",
        "                data.append({\"emotion\": emotion, \"file_path\": formatted_filename})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#remove calm emotion\n",
        "df = df[df.emotion != 'calm']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extracting waveforms and spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vQvjb4mASivk"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "from matplotlib.patches import Rectangle\n",
        "from torchaudio.utils import download_asset\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.random.manual_seed(42)\n",
        "\n",
        "def plot_waveform(waveform, sr, title=None, ax=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sr\n",
        "\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(num_channels, 1)\n",
        "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
        "    ax.set_xlim([0, time_axis[-1]])\n",
        "    ax.set_title(title)\n",
        "\n",
        "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(1, 1)\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pull_wave_and_spec():\n",
        "    base_dir = \"data/ravdess-emotional-speech-audio/versions/1\"\n",
        "    output_dir = \"speech\"\n",
        "\n",
        "    for actor in os.listdir(base_dir):\n",
        "        actor_path = os.path.join(base_dir, actor)\n",
        "        if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
        "            actor_num = int(actor.split(\"_\")[1])\n",
        "            if actor_num > 22: # change actor number here because it keeps crashing midway\n",
        "                print(f\"Processing {actor}...\")\n",
        "                for file in os.listdir(actor_path):\n",
        "                    if file.endswith(\".wav\"):\n",
        "                        SAMPLE_SPEECH = os.path.join(actor_path, file)\n",
        "                        SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_SPEECH)\n",
        "\n",
        "                        # Define transform\n",
        "                        spectrogram = T.Spectrogram(n_fft=512)\n",
        "\n",
        "                        # Perform transform\n",
        "                        spec = spectrogram(SPEECH_WAVEFORM)\n",
        "                        \n",
        "                        fig, ax = plt.subplots()\n",
        "                        plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=None, ax=ax)\n",
        "                        waveform_path = os.path.join(output_dir, f\"{actor}_{file}_waveform.png\")\n",
        "                        plt.savefig(waveform_path)\n",
        "                        plt.close(fig)\n",
        "\n",
        "                        # Create figure for spectrogram\n",
        "                        fig, ax = plt.subplots()\n",
        "                        plot_spectrogram(spec[0], title=None, ax=ax)\n",
        "                        spectrogram_path = os.path.join(output_dir, f\"{actor}_{file}_spectrogram.png\")\n",
        "                        plt.savefig(spectrogram_path)\n",
        "                        plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connecting speech waveforms and spectogram to pd dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   emotion                          file_path  \\\n",
            "0    angry  Actor_16_03-01-05-01-02-01-16.wav   \n",
            "1  fearful  Actor_16_03-01-06-01-02-02-16.wav   \n",
            "2  fearful  Actor_16_03-01-06-02-01-02-16.wav   \n",
            "3    angry  Actor_16_03-01-05-02-01-01-16.wav   \n",
            "4  disgust  Actor_16_03-01-07-01-01-01-16.wav   \n",
            "\n",
            "                                  spectrogram_tensor  \n",
            "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define base path for spectrogram images\n",
        "image_dir = \"speech\"  # Directory where spectrogram images are stored\n",
        "\n",
        "# Define transformations (convert images to tensors)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  \n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
        "])\n",
        "\n",
        "# Function to load spectrogram as tensor\n",
        "def load_spectrogram_tensor(file_path):\n",
        "    filename = os.path.basename(file_path)\n",
        "    spectrogram_img_path = os.path.join(image_dir, f\"{filename}_spectrogram.png\")  # Construct spectrogram path\n",
        "    \n",
        "    # Load image if it exists, else return None\n",
        "    if os.path.exists(spectrogram_img_path):\n",
        "        image = Image.open(spectrogram_img_path).convert(\"L\")  # Convert to grayscale\n",
        "        return transform(image)  # Convert to tensor\n",
        "    return None  # If file doesn't exist, return None\n",
        "\n",
        "# Apply function to extract spectrogram tensors\n",
        "df[\"spectrogram_tensor\"] = df[\"file_path\"].apply(load_spectrogram_tensor)\n",
        "\n",
        "# Display updated DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   emotion                          file_path  \\\n",
            "0    angry  Actor_16_03-01-05-01-02-01-16.wav   \n",
            "1  fearful  Actor_16_03-01-06-01-02-02-16.wav   \n",
            "2  fearful  Actor_16_03-01-06-02-01-02-16.wav   \n",
            "3    angry  Actor_16_03-01-05-02-01-01-16.wav   \n",
            "4  disgust  Actor_16_03-01-07-01-01-01-16.wav   \n",
            "\n",
            "                                  spectrogram_tensor  \\\n",
            "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "\n",
            "                                     waveform_tensor  \n",
            "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define base path for waveform images\n",
        "image_dir = \"speech\"  # Directory where waveform images are stored\n",
        "\n",
        "# Define transformations (convert images to tensors)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  \n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
        "])\n",
        "\n",
        "# Function to load waveform as tensor\n",
        "def load_waveform_tensor(file_path):\n",
        "    filename = os.path.basename(file_path)  # Extract filename (e.g., \"Actor_01_03-01-01-01-01-01-01.wav\")\n",
        "    waveform_img_path = os.path.join(image_dir, f\"{filename}_waveform.png\")  # Construct waveform path\n",
        "    \n",
        "    # Load image if it exists, else return None\n",
        "    if os.path.exists(waveform_img_path):\n",
        "        image = Image.open(waveform_img_path).convert(\"L\")  # Convert to grayscale\n",
        "        return transform(image)  # Convert to tensor\n",
        "    return None  # If file doesn't exist, return None\n",
        "\n",
        "# Apply function to extract waveform tensors\n",
        "df[\"waveform_tensor\"] = df[\"file_path\"].apply(load_waveform_tensor)\n",
        "\n",
        "# Display updated DataFrame\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 480, 640])\n",
            "torch.Size([1, 480, 640])\n"
          ]
        }
      ],
      "source": [
        "print(df.iloc[0, 2].shape)\n",
        "print(df.iloc[0, 3].shape)\n",
        "\n",
        "#combined = torch.cat([tensor1, tensor2], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['emotion'], random_state=42)\n",
        "torch.manual_seed(42);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "998"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.iloc[0]['spectrogram_tensor']\n",
        "\n",
        "emotion_to_num = {\n",
        "    \"neutral\": 0,\n",
        "    \"happy\": 1,\n",
        "    \"sad\": 2,\n",
        "    \"angry\": 3,\n",
        "    \"fearful\": 4,\n",
        "    \"disgust\": 5,\n",
        "    \"surprised\": 6\n",
        "}\n",
        "\n",
        "train_set = []\n",
        "test_set = []\n",
        "\n",
        "#create train set \n",
        "for i in range(len(train_df)):\n",
        "  #concat the spectogram and waveform tensors into one \n",
        "  combined = torch.cat([train_df.iloc[i]['spectrogram_tensor'], train_df.iloc[i]['waveform_tensor']], dim=0)\n",
        "  label_tensor = torch.tensor(emotion_to_num[train_df.iloc[i]['emotion']], dtype=torch.long)\n",
        "  #save to dataset \n",
        "  train_set.append((combined, label_tensor))\n",
        "\n",
        "#create test set \n",
        "for i in range(len(test_df)):\n",
        "  #concat the spectogram and waveform tensors into one \n",
        "  combined = torch.cat([test_df.iloc[i]['spectrogram_tensor'], test_df.iloc[i]['waveform_tensor']], dim=0)\n",
        "  label_tensor = torch.tensor(emotion_to_num[test_df.iloc[i]['emotion']], dtype=torch.long)\n",
        "  test_set.append((combined, label_tensor))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32  # <-- Please change this as necessary\n",
        "NUM_WORKERS = 0  # <-- Use more workers for more CPU threads\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3, stride=1, padding=1)  # (16, 480, 640)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # (16, 240, 320)\n",
        "        self.fc1 = nn.Linear(16 * 240 * 320, 128)  # Hidden layer\n",
        "        self.fc2 = nn.Linear(128, 7)  # Output layer for 7 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Conv + ReLU + Pooling\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten for the FC layer\n",
        "        x = F.relu(self.fc1(x))  # Hidden layer\n",
        "        x = self.fc2(x)  # Output logits (no softmax needed before CrossEntropyLoss)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm \n",
        "import numpy as np\n",
        "\n",
        "def train(train_loader, model, optimizer, criterion, n_epochs=10, **kwargs):\n",
        "    ### Define your training loop here\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for i in tqdm(range(n_epochs)):\n",
        "          print(i)\n",
        "          batch_losses = []\n",
        "          for j, data in enumerate(train_loader, 0):  \n",
        "            \n",
        "            x_batch, y_batch = data\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            y_predictions = model(x_batch)\n",
        "            loss = criterion(y_predictions, y_batch)\n",
        "            loss.backward() \n",
        "            optimizer.step()\n",
        "            \n",
        "            batch_losses.append(loss.item())\n",
        "          \n",
        "          losses.append(np.mean(batch_losses))\n",
        "          print(\"Last loss: \" + str(losses[-1]))\n",
        "    return losses "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "mymodel = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mymodel.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [03:18<13:13, 198.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last loss: 72.27686069905758\n",
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [06:02<08:54, 178.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last loss: 5.895802643150091\n",
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [08:37<05:35, 167.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last loss: 1.9237794429063797\n",
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [11:10<02:41, 161.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last loss: 1.945733454078436\n",
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [13:45<00:00, 165.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last loss: 1.9475602507591248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "\n",
        "losses = train(train_loader, mymodel, optimizer, criterion, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x15247f7c0>]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxaElEQVR4nO3deXRUZZ7/8U+FbGCSikSpIhCWbpDNZhEEozYqRjkoHBjCji2jdDvtREagHY/8ozOn+3ScnhmxnXHU6bZhztghgGzCsIhBwigBIZARQWmwWcKS0PJrKglLiMn9/XG7ShISSCVV9dTyfp1zTx6qblV977mnrI/3e+99HJZlWQIAAAiRONMFAACA2EL4AAAAIUX4AAAAIUX4AAAAIUX4AAAAIUX4AAAAIUX4AAAAIUX4AAAAIRVvuoCmGhoadObMGaWmpsrhcJguBwAAtIJlWaqurlZmZqbi4m58bCPswseZM2eUlZVlugwAANAG5eXl6t69+w3XCbvwkZqaKskuPi0tzXA1AACgNaqqqpSVleX7Hb+RsAsf3lZLWloa4QMAgAjTmlMmOOEUAACEFOEDAACEFOEDAACEFOEDAACEFOEDAACEFOEDAACEFOEDAACEFOEDAACEFOEDAACEFOEDAACEFOEDAACEFOEDAACEVNhNLBcsZ85Iv/2tdPmylJ9vuhoAAGJXzBz5OH1aeuUV6Y03pJoa09UAABC7YiZ8jBgh9e0rXbokrV1ruhoAAGJXzIQPh0OaPdse//73ZmsBACCWxUz4kL4LH1u3SufOma0FAIBYFVPho08faeRIqb5eWr7cdDUAAMSmmAofEq0XAABMi7nwMX261KGDtHu3dPSo6WoAAIg9foWPXr16yeFwXLfk5eVJkq5cuaK8vDxlZGQoJSVFubm5qqysDErhbeVySTk59rigwGwtAADEIr/Cx549e3T27FnfsnXrVknS1KlTJUkLFizQ+vXrtXLlShUXF+vMmTOaPHly4Ktup2tbL5ZlthYAAGKNw7La/vM7f/58bdiwQUeOHFFVVZVuv/12FRQUaMqUKZKkr776SgMGDFBJSYnuueeeVr1nVVWVnE6nPB6P0tLS2lraDVVX20dALl+W9uyx7wECAADazp/f7zaf83H16lW99957evrpp+VwOFRaWqq6ujrleHsakvr3768ePXqopKSkxfepra1VVVVVoyXYUlOliRPtMSeeAgAQWm0OH2vXrtWFCxf013/915KkiooKJSYmKj09vdF6LpdLFRUVLb5Pfn6+nE6nb8nKymprSX7xtl4KC+1LbwEAQGi0OXy8++67GjdunDIzM9tVwKJFi+TxeHxLeXl5u96vtcaOlTIypIoKadu2kHwkAABQG8PHiRMn9NFHH+nHP/6x7zG3262rV6/qwoULjdatrKyU2+1u8b2SkpKUlpbWaAmFhARp2jR7TOsFAIDQaVP4WLJkibp06aLHH3/c99jw4cOVkJCgoqIi32OHDx/WyZMnlZ2d3f5Kg8Dbelm92j75FAAABF+8vy9oaGjQkiVLNGfOHMXHf/dyp9OpuXPnauHChercubPS0tI0b948ZWdnt/pKl1C7916pVy/p+HFp/frvjoQAAIDg8fvIx0cffaSTJ0/q6aefvu65xYsXa/z48crNzdXo0aPldru1evXqgBQaDA6HNGuWPab1AgBAaLTrPh/BEIr7fFzr0CFp0CD7HJCKCqlz56B/JAAAUSck9/mIFgMHSkOHSnV10sqVpqsBACD6xXz4kL478fS998zWAQBALCB8SJo50z7/45NPpBMnTFcDAEB0I3xI6tZNevBBe8xMtwAABBfh4y+Y6RYAgNAgfPxFbq6UmCgdPCh9/rnpagAAiF6Ej79IT5fGj7fH3PMDAIDgIXxcw9t6WbZMamgwWwsAANGK8HGNxx6TnE7p1Clpxw7T1QAAEJ0IH9dITpamTLHHtF4AAAgOwkcT3tbL++9LtbVmawEAIBoRPpp44AH7vh8XLkgbN5quBgCA6EP4aCIuzr7jqUTrBQCAYCB8NOOJJ+y/GzZIHo/ZWgAAiDaEj2YMHiwNGmSf87FqlelqAACILoSPZjgcjW+3DgAAAofw0YJZs+y/H38snT5tthYAAKIJ4aMFPXtK999vTzJXWGi6GgAAogfh4wZovQAAEHiEjxuYOlWKj5f275e+/NJ0NQAARAfCxw1kZEjjxtljjn4AABAYhI+b8LZeCgrs8z8AAED7ED5uYsIEKSVFOnZMKikxXQ0AAJGP8HETnTpJkyfbY1ovAAC0H+GjFbytlxUrpLo6s7UAABDpCB+tMGaM5HJJ33wjffih6WoAAIhshI9WiI+XZsywx7ReAABoH8JHK3lbL+vWSTU1ZmsBACCSET5aacQIqW9f6dIlae1a09UAABC5CB+txEy3AAAEBuHDD97wsXWrdO6c2VoAAIhUhA8/9OkjjRwp1ddLy5ebrgYAgMhE+PATrRcAANqH8OGn6dOlDh2k3bulo0dNVwMAQOQhfPjJ5ZJycuxxQYHZWgAAiESEjza4tvXCTLcAAPiH8NEGkyZJHTtKf/iDVFpquhoAACKL3+Hj9OnTeuKJJ5SRkaGOHTvqBz/4gfbu3et73rIsvfzyy+ratas6duyonJwcHTlyJKBFm5aaKk2caI858RQAAP/4FT7+/Oc/67777lNCQoI2bdqkQ4cO6V//9V916623+tb51a9+pTfeeENvv/22du/erVtuuUVjx47VlStXAl68Sd7Wy7Jl0rffmq0FAIBI4rCs1p+18NJLL+nTTz/V//7v/zb7vGVZyszM1M9+9jO98MILkiSPxyOXy6WlS5dqhnd2thuoqqqS0+mUx+NRWlpaa0sLubo6qWtX6fx5acsW6dFHTVcEAIA5/vx++3Xk44MPPtCIESM0depUdenSRcOGDdNvfvMb3/PHjh1TRUWFcryXg0hyOp0aNWqUSkpKmn3P2tpaVVVVNVoiQUKCNG2aPab1AgBA6/kVPv74xz/qrbfeUt++fbVlyxY9++yz+ru/+zv913/9lySpoqJCkuRyuRq9zuVy+Z5rKj8/X06n07dkZWW1ZTuM8LZeVq+2J5wDAAA351f4aGho0F133aVf/vKXGjZsmJ555hn95Cc/0dtvv93mAhYtWiSPx+NbysvL2/xeoXbvvVKvXlJNjbR+velqAACIDH6Fj65du2rgwIGNHhswYIBOnjwpSXK73ZKkysrKRutUVlb6nmsqKSlJaWlpjZZI4XBIs2bZY1ovAAC0jl/h47777tPhw4cbPfaHP/xBPXv2lCT17t1bbrdbRUVFvuerqqq0e/duZWdnB6Dc8ONtvWzaZJ98CgAAbsyv8LFgwQLt2rVLv/zlL3X06FEVFBToP//zP5WXlydJcjgcmj9/vn7xi1/ogw8+0IEDB/Tkk08qMzNTkyZNCkb9xg0cKA0dal9uu3Kl6WoAAAh/foWPu+++W2vWrNGyZct055136uc//7lef/11zfb+77+kF198UfPmzdMzzzyju+++WzU1Ndq8ebOSk5MDXny4YKZbAABaz6/7fIRCpNzn41qnT0tZWfY8L8ePS3/pQgEAEDOCdp8PNK9bN+nBB+0xM90CAHBjhI8AYaZbAABah/ARILm5UmKidPCg9PnnpqsBACB8ET4CJD1dGj/eHnPiKQAALSN8BNC1M902NJitBQCAcEX4CKDHHrOPgJw6Je3YYboaAADCE+EjgJKTpSlT7DGtFwAAmkf4CDBv6+X996XaWrO1AAAQjggfATZ6tNS9u3ThgrRxo+lqAAAIP4SPAIuLk2bOtMe0XgAAuB7hIwi8rZcNGySPx2wtAACEG8JHEAweLA0aZJ/zsWqV6WoAAAgvhI8gcDiY6RYAgJYQPoJk1iz778cf27PeAgAAG+EjSHr2lO6/355krrDQdDUAAIQPwkcQ0XoBAOB6hI8gmjpVio+X9u+XvvzSdDUAAIQHwkcQZWRI48bZY45+AABgI3wEmbf1UlBgn/8BAECsI3wE2YQJUkqKdOyYVFJiuhoAAMwjfARZp07S5Mn2mNYLAACEj5Dwtl5WrJDq6szWAgCAaYSPEBgzRnK5pG++kT780HQ1AACYRfgIgfh4acYMe0zrBQAQ6wgfIeJtvaxbJ9XUmK0FAACTCB8hMmKE1LevdOmStHat6WoAADCH8BEizHQLAICN8BFC3vDx4YdSZaXZWgAAMIXwEUJ9+kgjR0oNDdLy5aarAQDADMJHiNF6AQDEOsJHiE2fLnXoIH32mXTkiOlqAAAIPcJHiLlcUk6OPS4oMFsLAAAmED4MuLb1wky3AIBYQ/gwYNIkqWNHu+2yd6/pagAACC3ChwGpqdLEifaYE08BALGG8GGIt/VSWCh9+63ZWgAACCXChyFjx0oZGfbNxrZtM10NAACh41f4+Id/+Ac5HI5GS//+/X3PX7lyRXl5ecrIyFBKSopyc3NVya08m5WQIE2bZo9pvQAAYonfRz4GDRqks2fP+pZPPvnE99yCBQu0fv16rVy5UsXFxTpz5owmT54c0IKjibf1snq1PeEcAACxIN7vF8THy+12X/e4x+PRu+++q4KCAo0ZM0aStGTJEg0YMEC7du3SPffc0/5qo8y990q9eknHj0vr19s3IAMAINr5feTjyJEjyszM1Pe+9z3Nnj1bJ0+elCSVlpaqrq5OOd47aEnq37+/evTooZKSkhbfr7a2VlVVVY2WWOFwSLNm2WNaLwCAWOFX+Bg1apSWLl2qzZs366233tKxY8f0wx/+UNXV1aqoqFBiYqLS09MbvcblcqmioqLF98zPz5fT6fQtWVlZbdqQSOVtvWzaJJ0/b7YWAABCwa/wMW7cOE2dOlWDBw/W2LFjtXHjRl24cEErVqxocwGLFi2Sx+PxLeXl5W1+r0g0cKA0dKh9ue3KlaarAQAg+Np1qW16erruuOMOHT16VG63W1evXtWFCxcarVNZWdnsOSJeSUlJSktLa7TEGma6BQDEknaFj5qaGn399dfq2rWrhg8froSEBBUVFfmeP3z4sE6ePKns7Ox2FxrNZs60z//45BPpxAnT1QAAEFx+hY8XXnhBxcXFOn78uHbu3Km/+qu/UocOHTRz5kw5nU7NnTtXCxcu1Mcff6zS0lI99dRTys7O5kqXm+jWTXroIXvMTLcAgGjnV/g4deqUZs6cqX79+mnatGnKyMjQrl27dPvtt0uSFi9erPHjxys3N1ejR4+W2+3W6tWrg1J4tGGmWwBArHBYVnj91FVVVcnpdMrj8cTU+R8ej+RySbW1UlmZNGSI6YoAAGg9f36/mdslTDid0vjx9pgTTwEA0YzwEUa8rZdly6SGBrO1AAAQLISPMPLYY1J6unTqlLRjh+lqAAAIDsJHGElKkqZMsce0XgAA0YrwEWa8rZf337dPPgUAINoQPsLM6NFS9+7ShQvSxo2mqwEAIPAIH2EmLs6+46lE6wUAEJ0IH2HI23rZsMG+/wcAANGE8BGGBg+WBg2yz/lYtcp0NQAABBbhIww5HMx0CwCIXoSPMDVrlv3344+l06fN1gIAQCARPsJUz57S/ffbk8wVFpquBgCAwCF8hDFaLwCAaET4CGNTp0rx8dL+/dKXX5quBgCAwCB8hLGMDGncOHvM0Q8AQLQgfIS5a1svlmW2FgAAAoHwEeYmTJBSUqTjx6WdO01XAwBA+xE+wlynTtLkyfaY1gsAIBoQPiKAt/WyYoVUV2e2FgAA2ovwEQHGjJFcLun8eWnLFtPVAADQPoSPCBAfL82YYY9pvQAAIh3hI0J4Wy/r1knV1WZrAQCgPQgfEWLECKlvX+nyZWntWtPVAADQdoSPCMFMtwCAaEH4iCDe8LF1q1RZabYWAADaivARQfr0kUaOlBoapOXLTVcDAEDbED4iDK0XAECkI3xEmOnTpQ4dpM8+k44cMV0NAAD+I3xEGJdLysmxxwUFZmsBAKAtCB8RiJluAQCRjPARgSZNkjp2tNsue/eargYAAP8QPiJQaqo0caI95sRTAECkIXxEKG/rpbBQ+vZbs7UAAOAPwkeEGjtWysiwbza2bZvpagAAaD3CR4RKSJCmTbPHtF4AAJGE8BHBvK2X1aulS5fM1gIAQGu1K3y8+uqrcjgcmj9/vu+xK1euKC8vTxkZGUpJSVFubq4qmYgkKO69V+rVS6qpkdavN10NAACt0+bwsWfPHr3zzjsaPHhwo8cXLFig9evXa+XKlSouLtaZM2c0efLkdheK6zHTLQAgErUpfNTU1Gj27Nn6zW9+o1tvvdX3uMfj0bvvvqvXXntNY8aM0fDhw7VkyRLt3LlTu3btCljR+I43fGzaJJ0/b7YWAABao03hIy8vT48//rhyvPf5/ovS0lLV1dU1erx///7q0aOHSkpK2lcpmjVggDRsmH257cqVpqsBAODm4v19QWFhofbt26c9e/Zc91xFRYUSExOVnp7e6HGXy6WKiopm36+2tla1tbW+f1dVVflbUsybPVvav99uvfz0p6arAQDgxvw68lFeXq7nn39ev//975WcnByQAvLz8+V0On1LVlZWQN43lsyYYZ//8ckn0okTpqsBAODG/AofpaWlOnfunO666y7Fx8crPj5excXFeuONNxQfHy+Xy6WrV6/qwoULjV5XWVkpt9vd7HsuWrRIHo/Ht5SXl7d5Y2JVt27SQw/ZY2a6BQCEO7/Cx8MPP6wDBw6orKzMt4wYMUKzZ8/2jRMSElRUVOR7zeHDh3Xy5EllZ2c3+55JSUlKS0trtMB/zHQLAIgUfp3zkZqaqjvvvLPRY7fccosyMjJ8j8+dO1cLFy5U586dlZaWpnnz5ik7O1v33HNP4KrGdXJzpb/9W+ngQenzz6UhQ0xXBABA8wJ+h9PFixdr/Pjxys3N1ejRo+V2u7V69epAfwyacDql8ePtMff8AACEM4dlhddB+qqqKjmdTnk8HlowflqzRpo8Were3T7xNI6b5wMAQsSf329+nqLIY49J6enSqVPSjh2mqwEAoHmEjyiSlCRNmWKPab0AAMIV4SPKeK96ef996Zp7twEAEDYIH1Fm9Gj7nI8LF6SNG01XAwDA9QgfUSYuTpo50x7TegEAhCPCRxTytl42bLCPgAAAEE4IH1Fo8GBp0CD7nI9Vq0xXAwBAY4SPKORwNL7dOgAA4YTwEaVmzbL/bt8unT5ttBQAABohfESpnj2l+++3J5lbtsx0NQAAfIfwEcVovQAAwhHhI4pNnSrFx0tlZdKhQ6arAQDARviIYhkZ0rhx9pijHwCAcEH4iHLe1ktBgX3+BwAAphE+otyECVJKinT8uLRzp+lqAAAgfES9Tp2kyZPtMa0XAEA4IHzEAG/rZcUKqa7ObC0AABA+YsCYMZLLJZ0/L23ZYroaAECsI3zEgPh4acYMe0zrBQBgGuEjRnhbL+vWSdXVZmsBAMQ2wkeMGDFC6ttXunxZWrvWdDUAgFhG+IgRzHQLAAgXhI8Y4g0fW7dKlZVmawEAxC7CRwzp00caOVJqaJCWLzddDQAgVhE+YgytFwCAaYSPGDN9utShg/TZZ9KRI6arAQDEIsJHjHG5pJwce1xQYLYWAEBsInzEoGtbL8x0CwAINcJHDJo0SerY0W677N1ruhoAQKwhfMSg1FRp4kR7zImnAIBQI3zEqCeesP8WFkrffmu2FgBAbCF8xKhHH5Vuu82+2di2baarAQDEEsJHjEpIkKZNs8e0XgAAoUT4iGHeq15Wr5YuXTJbCwAgdhA+Ylh2ttS7t1RTI61fb7oaAECsIHzEMIdDmjXLHtN6AQCECuEjxnlbL5s2SefPm60FABAb/Aofb731lgYPHqy0tDSlpaUpOztbmzZt8j1/5coV5eXlKSMjQykpKcrNzVUlc7eHtQEDpGHD7MttV640XQ0AIBb4FT66d++uV199VaWlpdq7d6/GjBmjiRMn6uDBg5KkBQsWaP369Vq5cqWKi4t15swZTZ48OSiFI3CY6RYAEEoOy2rf7B6dO3fWP//zP2vKlCm6/fbbVVBQoClTpkiSvvrqKw0YMEAlJSW65557WvV+VVVVcjqd8ng8SktLa09paKXTp6WsLHuel+PHpZ49TVcEAIg0/vx+t/mcj/r6ehUWFurixYvKzs5WaWmp6urqlOOdMlVS//791aNHD5WUlLT4PrW1taqqqmq0ILS6dZMeesgeM9MtACDY/A4fBw4cUEpKipKSkvTTn/5Ua9as0cCBA1VRUaHExESlp6c3Wt/lcqmioqLF98vPz5fT6fQtWVlZfm8E2o+ZbgEAoeJ3+OjXr5/Kysq0e/duPfvss5ozZ44OHTrU5gIWLVokj8fjW8rLy9v8Xmi73FwpKUk6eFD6/HPT1QAAopnf4SMxMVF9+vTR8OHDlZ+fryFDhujXv/613G63rl69qgsXLjRav7KyUm63u8X3S0pK8l09410Qek6nNH68PX7vPbO1AACiW7vv89HQ0KDa2loNHz5cCQkJKioq8j13+PBhnTx5UtnZ2e39GISAt/WybJlUX2+2FgBA9Ir3Z+VFixZp3Lhx6tGjh6qrq1VQUKDt27dry5Ytcjqdmjt3rhYuXKjOnTsrLS1N8+bNU3Z2dquvdIFZjz0mpafbV7/s2PHdSagAAASSX+Hj3LlzevLJJ3X27Fk5nU4NHjxYW7Zs0SOPPCJJWrx4seLi4pSbm6va2lqNHTtW//Ef/xGUwhF4SUnSlCnSb39rn3hK+AAABEO77/MRaNznw6zt2+3Q4XRKFRVScrLpigAAkSAk9/lAdBo9WureXfJ4pI0bTVcDAIhGhA80EhcnzZxpj7ndOgAgGAgfuI73qpcNG6QmV04DANBuhA9cZ/BgadAg6epVadUq09UAAKIN4QPXcTiY6RYAEDyEDzRr1iz77/bt9n0/AAAIFMIHmtWzp3T//fYkc8uWma4GABBNCB9oEa0XAEAwED7QoqlTpfh4qaxMasfExQAANEL4QIsyMqRx4+wxRz8AAIFC+MANeVsvBQX2+R8AALQX4QM3NGGClJIiHT8u7dxpuhoAQDQgfOCGOnWSJk+2x7ReAACBQPjATXlbLytWSHV1ZmsBAEQ+wgduaswYyeWSzp+XtmwxXQ0AINIRPnBT8fHSjBn2mNYLAKC9CB9oFW/rZd06qbrabC0AgMhG+ECrjBgh9e0rXb4srV1ruhoAQCQjfKBVmOkWABAohA+0mjd8bN0qVVaarQUAELkIH2i1Pn2kkSOlhgZp+XLT1QAAIhXhA36h9QIAaC/CB/wyfbrUoYP02WfSkSOmqwEARCLCB/zickmPPGKPCwrM1gIAiEyED/jt2tYLM90CAPxF+IDfJk2yJ5w7ckTau9d0NQCASEP4gN9SUqSJE+0xJ54CAPxF+ECbeFsvhYXSt9+arQUAEFkIH2iTRx+VbrvNvtnYtm2mqwEARBLCB9okIUGaNs0e03oBAPiD8IE287ZeVq+WLl0yWwsAIHIQPtBm2dlS795STY20fr3pagAAkYLwgTZzOKRZs+zxe++ZrQUAEDkIH2gXb+tl82bpm2/M1gIAiAyED7TLgAHSsGH25bYrV5quBgAQCQgfaDdmugUA+IPwgXabMcM+/+PTT6Xjx01XAwAId36Fj/z8fN19991KTU1Vly5dNGnSJB0+fLjROleuXFFeXp4yMjKUkpKi3NxcVVZWBrRohJdu3aSHHrLHzHQLALgZv8JHcXGx8vLytGvXLm3dulV1dXV69NFHdfHiRd86CxYs0Pr167Vy5UoVFxfrzJkzmjx5csALR3hhplsAQGs5LKvtPxV/+tOf1KVLFxUXF2v06NHyeDy6/fbbVVBQoClTpkiSvvrqKw0YMEAlJSW65557bvqeVVVVcjqd8ng8SktLa2tpCDGPR3K5pNpaaf9+aehQ0xUBAELJn9/vdp3z4fF4JEmdO3eWJJWWlqqurk45OTm+dfr3768ePXqopKSk2feora1VVVVVowWRx+mUxo+3x5x4CgC4kTaHj4aGBs2fP1/33Xef7rzzTklSRUWFEhMTlZ6e3mhdl8ulioqKZt8nPz9fTqfTt2RlZbW1JBjmbb0sWybV15utBQAQvtocPvLy8vTFF1+osLCwXQUsWrRIHo/Ht5SXl7fr/WDOY49J6enS6dPSjh2mqwEAhKs2hY/nnntOGzZs0Mcff6zu3bv7Hne73bp69aouXLjQaP3Kykq53e5m3yspKUlpaWmNFkSmpCTpL6f60HoBALTIr/BhWZaee+45rVmzRtu2bVPv3r0bPT98+HAlJCSoqKjI99jhw4d18uRJZWdnB6ZihDVv6+X996UrV8zWAgAIT/H+rJyXl6eCggKtW7dOqampvvM4nE6nOnbsKKfTqblz52rhwoXq3Lmz0tLSNG/ePGVnZ7fqShdEvtGjpe7dpVOnpI0bJa6yBgA05deRj7feeksej0cPPvigunbt6luWL1/uW2fx4sUaP368cnNzNXr0aLndbq1evTrghSM8xcVJM2faY1ovAIDmtOs+H8HAfT4i3//9n32fj8REqbLSPgkVABDdQnafD6A5gwdLgwZJV69Kq1aZrgYAEG4IHwg4h4OZbgEALSN8IChmzbL/bt9u3/cDAAAvwgeComdP6f777Unmli0zXQ0AIJwQPhA0tF4AAM0hfCBopk6V4uOlsjLp0CHT1QAAwgXhA0GTkSGNG2ePOfoBAPAifCCovK2XggL7/A8AAAgfCKoJE6SUFOn4cWnnTtPVAADCAeEDQdWp03fzu9B6AQBIhA+EgLf1smKFVFdnthYAgHmEDwTdmDGSyyWdPy9t2WK6GgCAaYQPBF18vDRjhj2m9QIAIHwgJLytl3XrpOpqs7UAAMwifCAkRoyQ7rhDunxZWrvWdDUAAJMIHwgJZroFAHgRPhAy3plut26VKivN1gIAMIfwgZDp00caNUpqaJCWLzddDQDAFMIHQorWCwCA8IGQmj5d6tBB+uwz6cgR09UAAEwgfCCkunSRHnnEHhcUmK0FAGAG4QMhd23rhZluASD2ED4QcpMm2RPOHTki7dljuhoAQKgRPhByKSnSxIn2mBNPASD2ED5ghLf1Ulgoffut2VoAAKFF+IARjz4q3XabdO6cVFRkuhoAQCgRPmBEQoI0bZo9pvUCALGF8AFjvK2XNWukS5fM1gIACB3CB4zJzpZ695ZqaqQPPjBdDQAgVAgfMMbh+G6yOVovABA7CB8wytt62bxZ+uYbs7UAAEKD8AGjBgyQhg2zL7ddudJ0NQCAUCB8wDhmugWA2EL4gHEzZtjnf3z6qXT8uOlqAADBRviAcd26SQ89ZI+Z6RYAoh/hA2GBmW4BIHYQPhAWcnOlpCTp0CHp//7PdDUAgGDyO3zs2LFDEyZMUGZmphwOh9auXdvoecuy9PLLL6tr167q2LGjcnJydOTIkUDViyjldErjx9tjTjwFgOjmd/i4ePGihgwZojfffLPZ53/1q1/pjTfe0Ntvv63du3frlltu0dixY3XlypV2F4vo5m29LFsm1debrQUAEDwOy2p7h93hcGjNmjWaNGmSJPuoR2Zmpn72s5/phRdekCR5PB65XC4tXbpUM2bMuOl7VlVVyel0yuPxKC0tra2lIQLV1kput3ThgrRt23cnoQIAwp8/v98BPefj2LFjqqioUE5Oju8xp9OpUaNGqaSkpNnX1NbWqqqqqtGC2JSUJE2ZYo9pvQBA9Apo+KioqJAkuVyuRo+7XC7fc03l5+fL6XT6lqysrECWhAjjbb28/75Epw4AopPxq10WLVokj8fjW8rLy02XBINGj5a6d5c8HmnjRtPVAACCIaDhw+12S5IqKysbPV5ZWel7rqmkpCSlpaU1WhC74uKkmTPtMa0XAIhOAQ0fvXv3ltvtVlFRke+xqqoq7d69W9nZ2YH8KEQxb+tlwwb75FMAQHTxO3zU1NSorKxMZWVlkuyTTMvKynTy5Ek5HA7Nnz9fv/jFL/TBBx/owIEDevLJJ5WZmem7Iga4mcGDpUGDpKtXpVWrTFcDAAg0v8PH3r17NWzYMA0bNkyStHDhQg0bNkwvv/yyJOnFF1/UvHnz9Mwzz+juu+9WTU2NNm/erOTk5MBWjqjlcDDTLQBEs3bd5yMYuM8HJOnECalXLzuIlJfbk88BAMKXsft8AIHSs6d0//32JHPLlpmuBgAQSIQPhC1aLwAQnQgfCFtTp0rx8VJZmT3bLQAgOhA+ELYyMqRx4+wxRz8AIHoQPhDWvK2XggL7/A8AQOQjfCCsTZggpaRIx49LO3eargYAEAiED4S1Tp2k3Fx7TOsFAKID4QNhz9t6WbFCqqszWwsAoP0IHwh7Y8ZIbrd0/ry0ZYvpagAA7UX4QNjr0EGaMcMe03oBgMhH+EBE8LZe1q2TqqvN1gIAaB/CByLC8OHSHXdIly9LL78sffKJ9Oc/m64KANAWhA9EBIdDeuIJe/z669IPfyh17ixlZkqPPCLNny/95jf25bgej8lKAQA3E2+6AKC15s+Xamulffukgwelkyels2ft5aOPGq/brZt0553SoEHfLQMHSqmpRkoHAFzDYVnhdd9If6bkRWyrqrLnfDl4sPFy+nTLr+nRo3Eg8YaSW24JXd0AEI38+f0mfCDqXLjQfCg5e7bl1/TqZQeRa4+W9O9v3+QMAHBzhA+gGf/v/10fSA4elM6da359h0P63veuP1LSv7+UnBza2gEg3BE+AD988831geSLL+ybmjUnLk76/vevP6fkjjukpKTQ1g4A4YLwAbSTZdlHRJo7UtLSJb4dOkh9+15/pOSOO6SEhNDWDwChRvgAgsSypIqK5kNJS5f4xsfbAaTpkZI+feznACAaED6AELMs6cyZ71o23kBy6FDLd2RNTJT69bv+SMn3v28fRQGASEL4AMKEZUnl5dcfJTl0SLp4sfnXJCXZJ7U2vfqmd2/7fBMACEeEDyDMNTTYN0m79gTXgwelL7+0byHfnI4dpQEDrj9S0rMnoQSAeYQPIELV10vHj19/pOTLL+27uzbnllu+CyXXHinJyrIvFwaAUCB8AFGmvl764x+vvxz48GHp6tXmX5Oaat+9temRkm7dCCUAAo/wAcSIb7+Vjh69/kjJ4cP2c81xOu1Q0vTqG7ebUAKg7QgfQIyrq5OOHLk+lPzhD/ZRlObceuv1R0kGDZK6dCGUALg5wgeAZtXW2gGkaSg5etQ+CbY5GRnfBZFrj5bcdltoawcQ3ggfAPxy5YrdqmkaSr7+2r5cuDldujR/pKRz59DWDiA8ED4ABMTly9JXXzW+cdrBg9KxYy2/xu1u/kiJ0xm6ugGEHuEDQFBdvGhf/tv0SMmJEy2/plu35m8r3/S/QDf7d1teEw6fEYz3DJfPiDbRvn2S/X18443AvifhA4AR1dV2KGl6pOTUKdOVAbhWv372Uc1A8uf3m2mtAARMaqo0cqS9XMvjsW8p7z060vR/eZpeTXOzf7flNbHyGcF4z9Z8RrSJ9u1LTzf7+YQPAEHndErZ2fYCAMwIAQAAQorwAQAAQipo4ePNN99Ur169lJycrFGjRumzzz4L1kcBAIAIEpTwsXz5ci1cuFCvvPKK9u3bpyFDhmjs2LE6d+5cMD4OAABEkKCEj9dee00/+clP9NRTT2ngwIF6++231alTJ/3ud78LxscBAIAIEvDwcfXqVZWWlionJ+e7D4mLU05OjkpKSgL9cQAAIMIE/FLbb775RvX19XK5XI0ed7lc+qqZO5rU1taqtrbW9++qqqpAlwQAAMKI8atd8vPz5XQ6fUtWVpbpkgAAQBAFPHzcdttt6tChgyorKxs9XllZKbfbfd36ixYtksfj8S3l5eWBLgkAAISRgIePxMREDR8+XEVFRb7HGhoaVFRUpOxmbm+YlJSktLS0RgsAAIheQbm9+sKFCzVnzhyNGDFCI0eO1Ouvv66LFy/qqaeeCsbHAQCACBKU8DF9+nT96U9/0ssvv6yKigoNHTpUmzdvvu4kVAAAEHscltV0fkmz/JmSFwAAhAd/fr/DblZbbxbiklsAACKH93e7Ncc0wi58VFdXSxKX3AIAEIGqq6vldDpvuE7YtV0aGhp05swZpaamyuFwBPS9q6qqlJWVpfLy8qhs6UT79knRv41sX+SL9m1k+yJfsLbRsixVV1crMzNTcXE3vpg27I58xMXFqXv37kH9jGi/pDfat0+K/m1k+yJftG8j2xf5grGNNzvi4WX8DqcAACC2ED4AAEBIxVT4SEpK0iuvvKKkpCTTpQRFtG+fFP3byPZFvmjfRrYv8oXDNobdCacAACC6xdSRDwAAYB7hAwAAhBThAwAAhBThAwAAhFTUhY8333xTvXr1UnJyskaNGqXPPvvshuuvXLlS/fv3V3Jysn7wgx9o48aNIaq0bfzZvqVLl8rhcDRakpOTQ1itf3bs2KEJEyYoMzNTDodDa9euvelrtm/frrvuuktJSUnq06ePli5dGvQ628Pfbdy+fft1+9DhcKiioiI0BfshPz9fd999t1JTU9WlSxdNmjRJhw8fvunrIuk72JZtjKTv4VtvvaXBgwf7bj6VnZ2tTZs23fA1kbT/JP+3MZL2X3NeffVVORwOzZ8//4brhXo/RlX4WL58uRYuXKhXXnlF+/bt05AhQzR27FidO3eu2fV37typmTNnau7cudq/f78mTZqkSZMm6Ysvvghx5a3j7/ZJ9h3szp4961tOnDgRwor9c/HiRQ0ZMkRvvvlmq9Y/duyYHn/8cT300EMqKyvT/Pnz9eMf/1hbtmwJcqVt5+82eh0+fLjRfuzSpUuQKmy74uJi5eXladeuXdq6davq6ur06KOP6uLFiy2+JtK+g23ZRilyvofdu3fXq6++qtLSUu3du1djxozRxIkTdfDgwWbXj7T9J/m/jVLk7L+m9uzZo3feeUeDBw++4XpG9qMVRUaOHGnl5eX5/l1fX29lZmZa+fn5za4/bdo06/HHH2/02KhRo6y/+Zu/CWqdbeXv9i1ZssRyOp0hqi6wJFlr1qy54TovvviiNWjQoEaPTZ8+3Ro7dmwQKwuc1mzjxx9/bEmy/vznP4ekpkA6d+6cJckqLi5ucZ1I+w421ZptjOTvoWVZ1q233mr99re/bfa5SN9/Xjfaxkjdf9XV1Vbfvn2trVu3Wg888ID1/PPPt7iuif0YNUc+rl69qtLSUuXk5Pgei4uLU05OjkpKSpp9TUlJSaP1JWns2LEtrm9SW7ZPkmpqatSzZ09lZWXdNN1Hmkjaf+01dOhQde3aVY888og+/fRT0+W0isfjkSR17ty5xXUifR+2ZhulyPwe1tfXq7CwUBcvXlR2dnaz60T6/mvNNkqRuf/y8vL0+OOPX7d/mmNiP0ZN+Pjmm29UX18vl8vV6HGXy9Vif7yiosKv9U1qy/b169dPv/vd77Ru3Tq99957amho0L333qtTp06FouSga2n/VVVV6fLly4aqCqyuXbvq7bff1qpVq7Rq1SplZWXpwQcf1L59+0yXdkMNDQ2aP3++7rvvPt15550trhdJ38GmWruNkfY9PHDggFJSUpSUlKSf/vSnWrNmjQYOHNjsupG6//zZxkjbf5JUWFioffv2KT8/v1Xrm9iPYTerLQInOzu7UZq/9957NWDAAL3zzjv6+c9/brAytFa/fv3Ur18/37/vvfdeff3111q8eLH++7//22BlN5aXl6cvvvhCn3zyielSgqa12xhp38N+/fqprKxMHo9H77//vubMmaPi4uIWf5wjkT/bGGn7r7y8XM8//7y2bt0a1ifGRk34uO2229ShQwdVVlY2eryyslJut7vZ17jdbr/WN6kt29dUQkKChg0bpqNHjwajxJBraf+lpaWpY8eOhqoKvpEjR4b1j/pzzz2nDRs2aMeOHerevfsN142k7+C1/NnGpsL9e5iYmKg+ffpIkoYPH649e/bo17/+td55553r1o3U/efPNjYV7vuvtLRU586d01133eV7rL6+Xjt27NC///u/q7a2Vh06dGj0GhP7MWraLomJiRo+fLiKiop8jzU0NKioqKjFXl52dnaj9SVp69atN+z9mdKW7Wuqvr5eBw4cUNeuXYNVZkhF0v4LpLKysrDch5Zl6bnnntOaNWu0bds29e7d+6avibR92JZtbCrSvocNDQ2qra1t9rlI238tudE2NhXu++/hhx/WgQMHVFZW5ltGjBih2bNnq6ys7LrgIRnaj0E7ldWAwsJCKykpyVq6dKl16NAh65lnnrHS09OtiooKy7Is60c/+pH10ksv+db/9NNPrfj4eOtf/uVfrC+//NJ65ZVXrISEBOvAgQOmNuGG/N2+f/zHf7S2bNliff3111Zpaak1Y8YMKzk52Tp48KCpTbih6upqa//+/db+/fstSdZrr71m7d+/3zpx4oRlWZb10ksvWT/60Y986//xj3+0OnXqZP393/+99eWXX1pvvvmm1aFDB2vz5s2mNuGm/N3GxYsXW2vXrrWOHDliHThwwHr++eetuLg466OPPjK1CS169tlnLafTaW3fvt06e/asb7l06ZJvnUj/DrZlGyPpe/jSSy9ZxcXF1rFjx6zPP//ceumllyyHw2F9+OGHlmVF/v6zLP+3MZL2X0uaXu0SDvsxqsKHZVnWv/3bv1k9evSwEhMTrZEjR1q7du3yPffAAw9Yc+bMabT+ihUrrDvuuMNKTEy0Bg0aZP3P//xPiCv2jz/bN3/+fN+6LpfLeuyxx6x9+/YZqLp1vJeVNl282zRnzhzrgQceuO41Q4cOtRITE63vfe971pIlS0Jetz/83cZ/+qd/sr7//e9bycnJVufOna0HH3zQ2rZtm5nib6K57ZLUaJ9E+newLdsYSd/Dp59+2urZs6eVmJho3X777dbDDz/s+1G2rMjff5bl/zZG0v5rSdPwEQ770WFZlhW84yoAAACNRc05HwAAIDIQPgAAQEgRPgAAQEgRPgAAQEgRPgAAQEgRPgAAQEgRPgAAQEgRPgAAQEgRPgAAQEgRPgAAQEgRPgAAQEgRPgAAQEj9f2VsgV1hC5/4AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt \n",
        "\n",
        "\n",
        "plt.plot(np.arange(len(losses)), losses, label='Loss Plot', color='blue')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_accuracy(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # We don't need gradients for evaluation\n",
        "        for images, labels in dataloader:\n",
        "            outputs = model(images)  # Get model predictions\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
        "            total += labels.size(0)  # Increment the total number of samples\n",
        "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "    accuracy = 100 * correct / total  # Accuracy as a percentage\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2530.07s - Error when getting str with custom provider: <pydevd_plugins.extensions.types.pydevd_plugin_pandas_types.PandasDataFrameTypeResolveProvider object at 0x152636a90>.\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_xml.py\", line 233, in str_from_providers\n",
            "    return self._get_str_from_provider(provider, o, context)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_xml.py\", line 215, in _get_str_from_provider\n",
            "    return provider.get_str(o)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_plugins/extensions/types/pydevd_plugin_pandas_types.py\", line 117, in get_str\n",
            "    return repr(df)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/core/frame.py\", line 1094, in __repr__\n",
            "    return self.to_string(**repr_params)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/core/frame.py\", line 1271, in to_string\n",
            "    return fmt.DataFrameRenderer(formatter).to_string(\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 1134, in to_string\n",
            "    string = string_formatter.to_string()\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/string.py\", line 30, in to_string\n",
            "    text = self._get_string_representation()\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/string.py\", line 45, in _get_string_representation\n",
            "    strcols = self._get_strcols()\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/string.py\", line 36, in _get_strcols\n",
            "    strcols = self.fmt.get_strcols()\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 615, in get_strcols\n",
            "    strcols = self._get_strcols_without_index()\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 881, in _get_strcols_without_index\n",
            "    fmt_values = self.format_col(i)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 895, in format_col\n",
            "    return format_array(\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 1330, in format_array\n",
            "    return fmt_obj.get_result()\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 1363, in get_result\n",
            "    fmt_values = self._format_strings()\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 1430, in _format_strings\n",
            "    fmt_values.append(f\" {_format(v)}\")\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 1410, in _format\n",
            "    return str(formatter(x))\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 222, in pprint_thing\n",
            "    result = _pprint_seq(\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 119, in _pprint_seq\n",
            "    r = [\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 120, in <listcomp>\n",
            "    pprint_thing(next(s), _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 222, in pprint_thing\n",
            "    result = _pprint_seq(\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 119, in _pprint_seq\n",
            "    r = [\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 120, in <listcomp>\n",
            "    pprint_thing(next(s), _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 222, in pprint_thing\n",
            "    result = _pprint_seq(\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 119, in _pprint_seq\n",
            "    r = [\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 120, in <listcomp>\n",
            "    pprint_thing(next(s), _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 232, in pprint_thing\n",
            "    result = as_escaped_string(thing)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/pandas/io/formats/printing.py\", line 208, in as_escaped_string\n",
            "    result = str(thing)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/torch/_tensor.py\", line 461, in __repr__\n",
            "    return torch._tensor_str._str(self, tensor_contents=tensor_contents)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/torch/_tensor_str.py\", line 677, in _str\n",
            "    return _str_intern(self, tensor_contents=tensor_contents)\n",
            "  File \"/opt/miniconda3/envs/cdd203/lib/python3.8/site-packages/torch/_tensor_str.py\", line 575, in _str_intern\n",
            "    if self.numel() == 0 and not self.is_sparse:\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "accuracy = calculate_accuracy(mymodel, test_loader)\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cdd203",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
