{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPKZMTSSSivj",
        "outputId": "abdd1516-f6ca-4b12-d7b0-444bbfa080f9"
      },
      "outputs": [],
      "source": [
        "# Download dataset. This is not needed if you have the dataset already\n",
        "\n",
        "# import kagglehub\n",
        "\n",
        "# path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "\n",
        "# print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning and Reformatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SnlbjELrSivk",
        "outputId": "798e7ac8-364b-4ba1-caa3-c1601f3584bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling rate: 48000 Hz\n"
          ]
        }
      ],
      "source": [
        "# Function to check the sampling rate of a wav file and valid file path\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "def check_sampling_rate(file_path):\n",
        "    try:\n",
        "        with contextlib.closing(wave.open(file_path, 'r')) as wav_file:\n",
        "            sample_rate = wav_file.getframerate()\n",
        "            print(f\"Sampling rate: {sample_rate} Hz\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "file_path = 'data/ravdess-emotional-speech-audio/versions/1/Actor_01/03-01-01-01-01-01-01.wav'\n",
        "check_sampling_rate(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1l5AM4UkSivk",
        "outputId": "dba92514-0d9b-4bfc-c309-37cc31b2846f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        emotion                          file_path\n",
            "0       neutral  Actor_01_03-01-01-01-01-01-01.wav\n",
            "1       neutral  Actor_01_03-01-01-01-01-02-01.wav\n",
            "2       neutral  Actor_01_03-01-01-01-02-01-01.wav\n",
            "3       neutral  Actor_01_03-01-01-01-02-02-01.wav\n",
            "4          calm  Actor_01_03-01-02-01-01-01-01.wav\n",
            "...         ...                                ...\n",
            "1435  surprised  Actor_24_03-01-08-01-02-02-24.wav\n",
            "1436  surprised  Actor_24_03-01-08-02-01-01-24.wav\n",
            "1437  surprised  Actor_24_03-01-08-02-01-02-24.wav\n",
            "1438  surprised  Actor_24_03-01-08-02-02-01-24.wav\n",
            "1439  surprised  Actor_24_03-01-08-02-02-02-24.wav\n",
            "\n",
            "[1440 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "File naming convention\n",
        "\n",
        "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
        "\n",
        "Filename identifiers\n",
        "\n",
        "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "\n",
        "Vocal channel (01 = speech, 02 = song).\n",
        "\n",
        "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "\n",
        "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "\n",
        "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "\n",
        "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "emotion_mapping = {\n",
        "    \"01\": \"neutral\",\n",
        "    \"02\": \"calm\",\n",
        "    \"03\": \"happy\",\n",
        "    \"04\": \"sad\",\n",
        "    \"05\": \"angry\",\n",
        "    \"06\": \"fearful\",\n",
        "    \"07\": \"disgust\",\n",
        "    \"08\": \"surprised\"\n",
        "}\n",
        "\n",
        "file_dir = \"data/ravdess-emotional-speech-audio/versions/1/\"\n",
        "\n",
        "data = []\n",
        "\n",
        "for actor in os.listdir(file_dir):\n",
        "    actor_path = os.path.join(file_dir, actor)\n",
        "    \n",
        "    if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
        "        actor_number = actor.split(\"_\")[-1]\n",
        "\n",
        "        for file in os.listdir(actor_path):\n",
        "            if file.endswith(\".wav\"):\n",
        "                emotion_code = file[6:8]\n",
        "                emotion = emotion_mapping.get(emotion_code, \"unknown\")\n",
        "                formatted_filename = f\"Actor_{actor_number}_{file}\"\n",
        "                data.append({\"emotion\": emotion, \"file_path\": formatted_filename})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#remove calm emotion\n",
        "df = df[df.emotion != 'calm']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extracting waveforms and spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vQvjb4mASivk"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "from matplotlib.patches import Rectangle\n",
        "from torchaudio.utils import download_asset\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.random.manual_seed(42)\n",
        "\n",
        "def plot_waveform(waveform, sr, title=None, ax=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sr\n",
        "\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(num_channels, 1)\n",
        "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
        "    ax.set_xlim([0, time_axis[-1]])\n",
        "    ax.set_title(title)\n",
        "\n",
        "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(1, 1)\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pull_wave_and_spec():\n",
        "    base_dir = \"data/ravdess-emotional-speech-audio/versions/1\"\n",
        "    output_dir = \"speech\"\n",
        "\n",
        "    for actor in os.listdir(base_dir):\n",
        "        actor_path = os.path.join(base_dir, actor)\n",
        "        if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
        "            actor_num = int(actor.split(\"_\")[1])\n",
        "            if actor_num > 22: # change actor number here because it keeps crashing midway\n",
        "                print(f\"Processing {actor}...\")\n",
        "                for file in os.listdir(actor_path):\n",
        "                    if file.endswith(\".wav\"):\n",
        "                        SAMPLE_SPEECH = os.path.join(actor_path, file)\n",
        "                        SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_SPEECH)\n",
        "\n",
        "                        # Define transform\n",
        "                        spectrogram = T.Spectrogram(n_fft=512)\n",
        "\n",
        "                        # Perform transform\n",
        "                        spec = spectrogram(SPEECH_WAVEFORM)\n",
        "                        \n",
        "                        fig, ax = plt.subplots()\n",
        "                        plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=None, ax=ax)\n",
        "                        waveform_path = os.path.join(output_dir, f\"{actor}_{file}_waveform.png\")\n",
        "                        plt.savefig(waveform_path)\n",
        "                        plt.close(fig)\n",
        "\n",
        "                        # Create figure for spectrogram\n",
        "                        fig, ax = plt.subplots()\n",
        "                        plot_spectrogram(spec[0], title=None, ax=ax)\n",
        "                        spectrogram_path = os.path.join(output_dir, f\"{actor}_{file}_spectrogram.png\")\n",
        "                        plt.savefig(spectrogram_path)\n",
        "                        plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connecting speech waveforms and spectogram to pd dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    emotion                          file_path  \\\n",
            "0   neutral  Actor_01_03-01-01-01-01-01-01.wav   \n",
            "1   neutral  Actor_01_03-01-01-01-01-02-01.wav   \n",
            "2   neutral  Actor_01_03-01-01-01-02-01-01.wav   \n",
            "3   neutral  Actor_01_03-01-01-01-02-02-01.wav   \n",
            "12    happy  Actor_01_03-01-03-01-01-01-01.wav   \n",
            "\n",
            "                                   spectrogram_tensor  \n",
            "0   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "1   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "2   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "3   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "12  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define base path for spectrogram images\n",
        "image_dir = \"speech\"  # Directory where spectrogram images are stored\n",
        "\n",
        "# Define transformations (convert images to tensors)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  \n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
        "])\n",
        "\n",
        "# Function to load spectrogram as tensor\n",
        "def load_spectrogram_tensor(file_path):\n",
        "    filename = os.path.basename(file_path)\n",
        "    spectrogram_img_path = os.path.join(image_dir, f\"{filename}_spectrogram.png\")  # Construct spectrogram path\n",
        "    \n",
        "    # Load image if it exists, else return None\n",
        "    if os.path.exists(spectrogram_img_path):\n",
        "        image = Image.open(spectrogram_img_path).convert(\"L\")  # Convert to grayscale\n",
        "        return transform(image)  # Convert to tensor\n",
        "    return None  # If file doesn't exist, return None\n",
        "\n",
        "# Apply function to extract spectrogram tensors\n",
        "df[\"spectrogram_tensor\"] = df[\"file_path\"].apply(load_spectrogram_tensor)\n",
        "\n",
        "# Display updated DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    emotion                          file_path  \\\n",
            "0   neutral  Actor_01_03-01-01-01-01-01-01.wav   \n",
            "1   neutral  Actor_01_03-01-01-01-01-02-01.wav   \n",
            "2   neutral  Actor_01_03-01-01-01-02-01-01.wav   \n",
            "3   neutral  Actor_01_03-01-01-01-02-02-01.wav   \n",
            "12    happy  Actor_01_03-01-03-01-01-01-01.wav   \n",
            "\n",
            "                                   spectrogram_tensor  \\\n",
            "0   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "1   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "2   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "3   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "12  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
            "\n",
            "                                      waveform_tensor  \n",
            "0   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "1   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "2   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "3   [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
            "12  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define base path for waveform images\n",
        "image_dir = \"speech\"  # Directory where waveform images are stored\n",
        "\n",
        "# Define transformations (convert images to tensors)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  \n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
        "])\n",
        "\n",
        "# Function to load waveform as tensor\n",
        "def load_waveform_tensor(file_path):\n",
        "    filename = os.path.basename(file_path)  # Extract filename (e.g., \"Actor_01_03-01-01-01-01-01-01.wav\")\n",
        "    waveform_img_path = os.path.join(image_dir, f\"{filename}_waveform.png\")  # Construct waveform path\n",
        "    \n",
        "    # Load image if it exists, else return None\n",
        "    if os.path.exists(waveform_img_path):\n",
        "        image = Image.open(waveform_img_path).convert(\"L\")  # Convert to grayscale\n",
        "        return transform(image)  # Convert to tensor\n",
        "    return None  # If file doesn't exist, return None\n",
        "\n",
        "# Apply function to extract waveform tensors\n",
        "df[\"waveform_tensor\"] = df[\"file_path\"].apply(load_waveform_tensor)\n",
        "\n",
        "# Display updated DataFrame\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
